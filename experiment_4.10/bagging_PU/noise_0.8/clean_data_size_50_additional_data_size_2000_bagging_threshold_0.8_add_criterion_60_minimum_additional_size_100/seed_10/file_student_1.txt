training accuracy when lambda=0.2
[0.619030303030303, 0.7584646464646465, 0.7874949495093991, 0.8031717171861668, 0.8136565656469326, 0.8231313131216801, 0.8291111111159276, 0.832080808071175, 0.8366060606205102, 0.8425050505050505, 0.8470303030447527, 0.8500606060461564, 0.8535757575805741, 0.8578989898893569, 0.8595959596104092, 0.8637979797883467, 0.8644040403992239, 0.8657979797835302, 0.8683838383693888, 0.8721212121308451, 0.8735959595911431, 0.8769494949543115, 0.8772525252477087, 0.8783636363684529, 0.8789292929437426, 0.8813333333188837, 0.8837373737470068, 0.8828484848484849, 0.8843434343482509, 0.8873737373592878, 0.8877171717027221, 0.8890909090957256, 0.8913131312986817, 0.892606060601244, 0.8926262626310791, 0.8939191919240085, 0.8942424242472408, 0.895636363621914, 0.8977373737325572, 0.8978585858537693, 0.8973737373881869, 0.8998989898845403, 0.8991717171572675, 0.9014949494804999, 0.90296969697933, 0.904080808080808, 0.9018787878739714, 0.904444444458894, 0.9036969696825201, 0.9041212121163956]
test accuracy when lambda=0.2
[0.3736, 0.379, 0.3868, 0.3885, 0.3922, 0.3913, 0.3937, 0.3915, 0.3921, 0.3917, 0.3942, 0.3909, 0.3976, 0.3946, 0.3923, 0.3912, 0.3957, 0.3973, 0.3955, 0.3949, 0.3943, 0.3947, 0.3975, 0.3969, 0.3966, 0.3986, 0.3951, 0.3984, 0.3963, 0.4006, 0.3971, 0.4013, 0.4027, 0.3924, 0.3978, 0.3938, 0.3973, 0.403, 0.3969, 0.3981, 0.3958, 0.3968, 0.3981, 0.398, 0.398, 0.4009, 0.3993, 0.3985, 0.4002, 0.3985]
training accuracy when lambda=0.3
[0.6045252525156195, 0.7450303030447526, 0.7732929292784797, 0.7881616161567996, 0.7978585858634024, 0.8061212121356617, 0.8112727272630942, 0.8175959595959595, 0.8229898989850825, 0.8249494949446784, 0.8290707070658906, 0.8338585858489528, 0.8366060606108772, 0.8377373737421903, 0.8403434343289847, 0.8434545454449124, 0.8479191919336415, 0.849393939408389, 0.8504242424097929, 0.8532525252428922, 0.8548484848629344, 0.8541616161567996, 0.858464646469463, 0.8587676767773098, 0.8596161616209782, 0.8627272727320893, 0.864929292938926, 0.8653939393794898, 0.8651717171572676, 0.8688282828138332, 0.8699191919240085, 0.8692525252669748, 0.8703636363684529, 0.8709494949398618, 0.8724040403992238, 0.8726060605916109, 0.8725656565801062, 0.8730707070562574, 0.8747878787782457, 0.8751313131361297, 0.876646464632015, 0.8752727272582776, 0.8762424242472407, 0.8762020202116533, 0.875454545468995, 0.8782424242424243, 0.8788888888792559, 0.8791717171765338, 0.8786060606205102, 0.8798181818133652]
test accuracy when lambda=0.3
[0.3806, 0.3791, 0.386, 0.399, 0.3932, 0.3961, 0.4007, 0.3907, 0.3968, 0.4019, 0.397, 0.3996, 0.3976, 0.398, 0.3962, 0.396, 0.3955, 0.3954, 0.4014, 0.3936, 0.3997, 0.3971, 0.4018, 0.3976, 0.3957, 0.4008, 0.3988, 0.3978, 0.3981, 0.3992, 0.4048, 0.3974, 0.4, 0.3947, 0.4, 0.3993, 0.4009, 0.4002, 0.3951, 0.3989, 0.4016, 0.4023, 0.3986, 0.4, 0.4013, 0.3966, 0.398, 0.3978, 0.3964, 0.3967]
training accuracy when lambda=0.4
[0.5491111111207442, 0.6708686868735034, 0.6999797979846145, 0.7118585858489528, 0.7207272727128231, 0.7247272727272728, 0.7319797979942476, 0.7377575757672088, 0.7404040403944073, 0.7434949495045826, 0.7462626262626263, 0.7510505050456885, 0.7525656565656565, 0.7530505050649546, 0.7581616161567997, 0.7578787878836044, 0.760060606050973, 0.7632525252428922, 0.7641010101154597, 0.7647676767580437, 0.7677171717219883, 0.7685858585906751, 0.7708888888744393, 0.7700606060461564, 0.7708686868638703, 0.7736565656710153, 0.7741212121260287, 0.7740808080856246, 0.774909090918724, 0.7762424242279746, 0.7769292929196598, 0.7766666666666666, 0.7787676767580437, 0.777434343448793, 0.7784242424290589, 0.7790707070562575, 0.7778181818229983, 0.7787474747474747, 0.7778181818326314, 0.7782222222270387, 0.7780404040259544, 0.7810303030254865, 0.779555555550739, 0.7786262626407122, 0.7816565656710153, 0.7788888888985219, 0.7782020202164698, 0.7761818181914513, 0.775919191928825, 0.7774747474602979]
test accuracy when lambda=0.4
[0.3767, 0.388, 0.3818, 0.3949, 0.3951, 0.3963, 0.3996, 0.3972, 0.404, 0.4016, 0.4016, 0.4031, 0.4065, 0.4042, 0.4051, 0.4038, 0.406, 0.4077, 0.4052, 0.4051, 0.4033, 0.4032, 0.4056, 0.4075, 0.4079, 0.4075, 0.4098, 0.4047, 0.4107, 0.4076, 0.4058, 0.4078, 0.4099, 0.405, 0.4045, 0.4083, 0.4064, 0.4027, 0.4049, 0.4074, 0.4055, 0.4054, 0.407, 0.4059, 0.4068, 0.406, 0.403, 0.4042, 0.4034, 0.4034]
