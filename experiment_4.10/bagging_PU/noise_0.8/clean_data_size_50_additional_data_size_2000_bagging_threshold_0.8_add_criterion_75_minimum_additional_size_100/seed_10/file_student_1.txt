training accuracy when lambda=0.2
[0.6091919191919192, 0.7573939393843063, 0.7872121212169377, 0.8015757575805741, 0.8097171717268048, 0.8183030302933972, 0.8231919191822861, 0.8318989898893568, 0.8334949495093991, 0.8396565656469326, 0.8438383838528335, 0.8454343434198939, 0.8502020202068368, 0.853474747479564, 0.8563636363732694, 0.8597373737229241, 0.8604444444299948, 0.8638585858489528, 0.8649494949398618, 0.8667676767580437, 0.8702222222077726, 0.8719797979701649, 0.8735959596055927, 0.8747878787734292, 0.8773737373689209, 0.8775959596007762, 0.8804242424097928, 0.8812727272823604, 0.8821010100961936, 0.8837373737421903, 0.8860606060461564, 0.8862020202068367, 0.8882424242327912, 0.8890505050360554, 0.8901818181914513, 0.8896767676623181, 0.8924242424097928, 0.89496969697933, 0.8933131313227644, 0.8956363636315471, 0.895494949499766, 0.8971717171717172, 0.8975555555603721, 0.8977777777922273, 0.8981212121260287, 0.899838383848017, 0.9003232323280489, 0.9008282828330993, 0.9011111111111111, 0.9015555555507391]
test accuracy when lambda=0.2
[0.3673, 0.3851, 0.3901, 0.3845, 0.3892, 0.3778, 0.3903, 0.3841, 0.3891, 0.396, 0.3932, 0.3899, 0.3921, 0.3913, 0.388, 0.3906, 0.3932, 0.3885, 0.3913, 0.3934, 0.3956, 0.3874, 0.3919, 0.3891, 0.3905, 0.3966, 0.3901, 0.3921, 0.3902, 0.394, 0.3933, 0.3905, 0.3937, 0.3866, 0.3924, 0.3949, 0.3904, 0.3928, 0.3879, 0.3905, 0.391, 0.3883, 0.395, 0.393, 0.395, 0.3909, 0.3936, 0.3953, 0.3848, 0.3918]
training accuracy when lambda=0.3
[0.5993333333188837, 0.7382020202020202, 0.7692323232371397, 0.7810909090957256, 0.7951717171765337, 0.7995757575613079, 0.8068080807936312, 0.8110707070658906, 0.8150101010004679, 0.8174747474747475, 0.8211313131409462, 0.8271111111062945, 0.8301414141317811, 0.8336363636508133, 0.8348080807984477, 0.8374747474699309, 0.8407272727224562, 0.8431313131457627, 0.8411313131313132, 0.844424242429059, 0.8487676767628602, 0.8492525252380756, 0.8497979797931633, 0.8537373737470068, 0.8551313131264966, 0.858464646479096, 0.8573535353487188, 0.858666666652217, 0.8599595959644125, 0.8607676767724933, 0.862626262621446, 0.8643434343289847, 0.8641414141558638, 0.866464646479096, 0.8652121212073047, 0.8649494949446784, 0.8666262626310791, 0.8691515151370656, 0.8688282828330993, 0.8693333333381499, 0.8696565656661988, 0.872383838388655, 0.8712525252669748, 0.8707676767532272, 0.8721616161616161, 0.872505050500234, 0.8712525252573418, 0.8724242424097928, 0.8733535353535353, 0.871050505040872]
test accuracy when lambda=0.3
[0.3769, 0.3802, 0.3814, 0.3881, 0.3964, 0.393, 0.393, 0.3954, 0.391, 0.3971, 0.3882, 0.3984, 0.3923, 0.3895, 0.3967, 0.3882, 0.3939, 0.3986, 0.3952, 0.3945, 0.3977, 0.3941, 0.391, 0.3982, 0.3933, 0.3937, 0.391, 0.399, 0.387, 0.394, 0.392, 0.3984, 0.3987, 0.3973, 0.3975, 0.3956, 0.3935, 0.393, 0.3958, 0.3927, 0.3921, 0.3984, 0.3958, 0.3931, 0.3946, 0.3942, 0.3937, 0.3948, 0.3985, 0.3947]
training accuracy when lambda=0.4
[0.5352525252477087, 0.6643434343530674, 0.6881818181673686, 0.7005454545502711, 0.7083434343578839, 0.7155151515007019, 0.7185252525252526, 0.7252323232226902, 0.7303030303030303, 0.7338585858585859, 0.7346666666618501, 0.7390101010004679, 0.7401818181962678, 0.7445252525397021, 0.747555555550739, 0.7496565656469326, 0.7513333333429664, 0.7504242424097928, 0.7524848484848485, 0.7533333333429664, 0.7541616161616161, 0.7574343434247104, 0.7577575757527593, 0.7586464646609142, 0.7581212121163956, 0.7602626262529932, 0.7588888888744393, 0.7617373737421903, 0.763494949490133, 0.7623030303030303, 0.7633939393794897, 0.764606060591611, 0.7658787878932375, 0.7632121212024882, 0.7634949495093991, 0.7637373737470068, 0.7660404040452206, 0.7636565656565657, 0.7640202020250185, 0.7660808080759915, 0.7656969697114193, 0.7648484848484849, 0.7638989899134395, 0.7640606060461564, 0.7662626262722594, 0.7658989898893568, 0.7631111111159277, 0.7655555555651886, 0.7624848484944815, 0.7617979798027963]
test accuracy when lambda=0.4
[0.3712, 0.3808, 0.3853, 0.3875, 0.3985, 0.3956, 0.3901, 0.392, 0.4018, 0.3969, 0.4009, 0.4019, 0.3943, 0.4012, 0.4017, 0.4014, 0.3975, 0.4044, 0.4, 0.4045, 0.4029, 0.4014, 0.3994, 0.4008, 0.4013, 0.3976, 0.3996, 0.4072, 0.4008, 0.4018, 0.3949, 0.4003, 0.399, 0.4016, 0.3989, 0.3984, 0.4021, 0.4029, 0.3994, 0.3984, 0.4053, 0.4, 0.3997, 0.4002, 0.4011, 0.3993, 0.3985, 0.3956, 0.3999, 0.3972]
